
@article{holt_object-based_2009,
	title = {Object-based detection and classification of {Vehicles} from high-resolution aerial photography},
	volume = {75},
	issn = {0099-1112},
	url = {https://iths.pure.elsevier.com/en/publications/object-based-detection-and-classification-of-vehicles-from-high-r},
	language = {English},
	number = {7},
	urldate = {2018-02-07},
	journal = {Photogrammetric Engineering and Remote Sensing},
	author = {Holt, Ashley C. and Seto, Edmund Y. W. and Rivard, Tom and Gong, Peng},
	month = jul,
	year = {2009},
	pages = {871--880},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/9SSHR54P/object-based-detection-and-classification-of-vehicles-from-high-r.html:text/html}
}

@article{ammour_deep_2017,
	title = {Deep {Learning} {Approach} for {Car} {Detection} in {UAV} {Imagery}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/2072-4292/9/4/312},
	doi = {10.3390/rs9040312},
	abstract = {This paper presents an automatic solution to the problem of detecting and counting cars in unmanned aerial vehicle (UAV) images. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimetres) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by segmenting the input image into small homogeneous regions, which can be used as candidate locations for car detection. Next, a window is extracted around each region, and deep learning is used to mine highly descriptive features from these windows. We use a deep convolutional neural network (CNN) system that is already pre-trained on huge auxiliary data as a feature extraction tool, combined with a linear support vector machine (SVM) classifier to classify regions into “car” and “no-car” classes. The final step is devoted to a fine-tuning procedure which performs morphological dilation to smooth the detected regions and fill any holes. In addition, small isolated regions are analysed further using a few sliding rectangular windows to locate cars more accurately and remove false positives. To evaluate our method, experiments were conducted on a challenging set of real UAV images acquired over an urban area. The experimental results have proven that the proposed method outperforms the state-of-the-art methods, both in terms of accuracy and computational time.},
	language = {en},
	number = {4},
	urldate = {2018-02-07},
	journal = {Remote Sensing},
	author = {Ammour, Nassim and Alhichri, Haikel and Bazi, Yakoub and Benjdira, Bilel and Alajlan, Naif and Zuair, Mansour},
	month = mar,
	year = {2017},
	keywords = {car counting, convolutional neural networks (CNNs), deep learning, mean-shift segmentation, support vector machines (SVM), UAV imagery},
	pages = {312},
	file = {Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/923NA9BY/Ammour et al. - 2017 - Deep Learning Approach for Car Detection in UAV Im.pdf:application/pdf;Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EW3VJ8KZ/312.html:text/html}
}

@article{luc_semantic_2016,
	title = {Semantic {Segmentation} using {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.08408},
	abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
	urldate = {2018-02-07},
	journal = {arXiv:1611.08408 [cs]},
	author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08408},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.08408 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/S23TNTUT/Luc et al. - 2016 - Semantic Segmentation using Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/BPKKTLN4/1611.html:text/html}
}

@article{audebert_segment-before-detect:_2017,
	title = {Segment-before-{Detect}: {Vehicle} {Detection} and {Classification} through {Semantic} {Segmentation} of {Aerial} {Images}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Segment-before-{Detect}},
	url = {http://www.mdpi.com/2072-4292/9/4/368},
	doi = {10.3390/rs9040368},
	abstract = {Like computer vision before, remote sensing has been radically changed by the introduction of deep learning and, more notably, Convolution Neural Networks. Land cover classification, object detection and scene understanding in aerial images rely more and more on deep networks to achieve new state-of-the-art results. Recent architectures such as Fully Convolutional Networks can even produce pixel level annotations for semantic mapping. In this work, we present a deep-learning based segment-before-detect method for segmentation and subsequent detection and classification of several varieties of wheeled vehicles in high resolution remote sensing images. This allows us to investigate object detection and classification on a complex dataset made up of visually similar classes, and to demonstrate the relevance of such a subclass modeling approach. Especially, we want to show that deep learning is also suitable for object-oriented analysis of Earth Observation data as effective object detection can be obtained as a byproduct of accurate semantic segmentation. First, we train a deep fully convolutional network on the ISPRS Potsdam and the NZAM/ONERA Christchurch datasets and show how the learnt semantic maps can be used to extract precise segmentation of vehicles. Then, we show that those maps are accurate enough to perform vehicle detection by simple connected component extraction. This allows us to study the repartition of vehicles in the city. Finally, we train a Convolutional Neural Network to perform vehicle classification on the VEDAI dataset, and transfer its knowledge to classify the individual vehicle instances that we detected.},
	language = {en},
	number = {4},
	urldate = {2018-02-07},
	journal = {Remote Sensing},
	author = {Audebert, Nicolas and Le Saux, Bertrand and Lefèvre, Sébastien},
	month = apr,
	year = {2017},
	keywords = {deep learning, object classification, semantic segmentation, vehicle detection},
	pages = {368},
	file = {Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/VRGTR9WP/Audebert et al. - 2017 - Segment-before-Detect Vehicle Detection and Class.pdf:application/pdf;Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/U2F94FGB/368.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2018-02-07},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	note = {arXiv: 1701.00160},
	keywords = {Computer Science - Learning},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {arXiv\:1701.00160 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/C4RTQVK5/Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/67UBH6X9/1701.html:text/html}
}

@article{arbelle_microscopy_2017,
	title = {Microscopy {Cell} {Segmentation} via {Adversarial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1709.05860},
	abstract = {We present a novel method for cell segmentation in microscopy images which is inspired by the Generative Adversarial Neural Network (GAN) approach. Our framework is built on a pair of two competitive artificial neural networks, with a unique architecture, termed Rib Cage, which are trained simultaneously and together define a min-max game resulting in an accurate segmentation of a given image. Our approach has two main strengths, similar to the GAN, the method does not require a formulation of a loss function for the optimization process. This allows training on a limited amount of annotated data in a weakly supervised manner. Promising segmentation results on real fluorescent microscopy data are presented. The code is freely available at: https://github.com/arbellea/DeepCellSeg.git},
	urldate = {2018-02-07},
	journal = {arXiv:1709.05860 [cs]},
	author = {Arbelle, Assaf and Raviv, Tammy Riklin},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.05860},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to IEEE International Symposium on Biomedical Imaging (ISBI) 2018},
	file = {arXiv\:1709.05860 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/PXI22QDJ/Arbelle and Raviv - 2017 - Microscopy Cell Segmentation via Adversarial Neura.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/8CSETM6L/1709.html:text/html}
}

@article{son_retinal_2017,
	title = {Retinal {Vessel} {Segmentation} in {Fundoscopic} {Images} with {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1706.09318},
	abstract = {Retinal vessel segmentation is an indispensable step for automatic detection of retinal diseases with fundoscopic images. Though many approaches have been proposed, existing methods tend to miss fine vessels or allow false positives at terminal branches. Let alone under-segmentation, over-segmentation is also problematic when quantitative studies need to measure the precise width of vessels. In this paper, we present a method that generates the precise map of retinal vessels using generative adversarial training. Our methods achieve dice coefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the state-of-the-art performance on both datasets.},
	urldate = {2018-02-07},
	journal = {arXiv:1706.09318 [cs]},
	author = {Son, Jaemin and Park, Sang Jun and Jung, Kyu-Hwan},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.09318},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: 9 pages, submitted to DLMIA 2017},
	file = {arXiv\:1706.09318 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/B6MJ2TAB/Son et al. - 2017 - Retinal Vessel Segmentation in Fundoscopic Images .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/49VZ8EA7/1706.html:text/html}
}

@misc{mundhenk_cars_nodate,
	title = {Cars {Overhead} {With} {Context} {Dataset} at {LLNL}},
	url = {http://gdo-datasci.ucllnl.org/cowc/},
	abstract = {A large diverse set of cars from overhead images, 
                                                  which are useful for training a deep learner to binary classify,
                                                  detect and count them.},
	urldate = {2018-02-07},
	author = {Mundhenk, T. Nathan},
	file = {Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/JK7BNH2V/cowc.html:text/html}
}

@misc{noauthor_2d_nodate,
	title = {2D {Semantic} {Labeling} - {ISPRS}},
	url = {http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html},
	urldate = {2018-02-07},
	file = {2D Semantic Labeling - ISPRS:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/YIAYBXVN/semantic-labeling.html:text/html}
}

@misc{noauthor_github_nodate,
	title = {{GitHub} - nightrome/really-awesome-gan: {A} list of papers on {Generative} {Adversarial} ({Neural}) {Networks}},
	url = {https://github.com/nightrome/really-awesome-gan},
	urldate = {2018-02-07},
	file = {GitHub - nightrome/really-awesome-gan\: A list of papers on Generative Adversarial (Neural) Networks:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EVI2KYE4/really-awesome-gan.html:text/html}
}

@article{razakarivony_vehicle_2015,
	title = {Vehicle {Detection} in {Aerial} {Imagery} : {A} small target detection benchmark},
	shorttitle = {Vehicle {Detection} in {Aerial} {Imagery}},
	url = {https://hal.archives-ouvertes.fr/hal-01122605},
	abstract = {This paper introduces VEDAI: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit different variabil-ities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by different people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for different settings of these algorithms, to illustrate the difficulties of the task and provide baseline comparisons.},
	urldate = {2018-02-07},
	journal = {Journal of Visual Communication and Image Representation, Elsevier},
	author = {Razakarivony, Sébastien and Jurie, Frédéric},
	month = mar,
	year = {2015},
	keywords = {Aerial Imagery, Database, Detection, Infrared Imagery, Low Resolution Images, Vehicles},
	file = {HAL PDF Full Text:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/IDUK6AKR/Razakarivony and Jurie - 2015 - Vehicle Detection in Aerial Imagery  A small targ.pdf:application/pdf}
}

@article{reed_generative_2016,
	title = {Generative {Adversarial} {Text} to {Image} {Synthesis}},
	url = {https://arxiv.org/abs/1605.05396},
	language = {en},
	urldate = {2018-02-18},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	month = may,
	year = {2016}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	urldate = {2018-02-18},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv\:1411.1784 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/A63UHF3E/Mirza and Osindero - 2014 - Conditional Generative Adversarial Nets.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/WZYG8SNE/1411.html:text/html}
}

@article{isola_image--image_2016,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2018-02-18},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/},
	file = {arXiv\:1611.07004 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/M96T4WQ4/Isola et al. - 2016 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/QPZLAU3V/1611.html:text/html}
}

@article{zhong_robust_2017,
	title = {Robust {Vehicle} {Detection} in {Aerial} {Images} {Based} on {Cascaded} {Convolutional} {Neural} {Networks}},
	volume = {17},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5751529/},
	doi = {10.3390/s17122720},
	abstract = {Vehicle detection in aerial images is an important and challenging task. Traditionally, many target detection models based on sliding-window fashion were developed and achieved acceptable performance, but these models are time-consuming in the detection phase. Recently, with the great success of convolutional neural networks (CNNs) in computer vision, many state-of-the-art detectors have been designed based on deep CNNs. However, these CNN-based detectors are inefficient when applied in aerial image data due to the fact that the existing CNN-based models struggle with small-size object detection and precise localization. To improve the detection accuracy without decreasing speed, we propose a CNN-based detection model combining two independent convolutional neural networks, where the first network is applied to generate a set of vehicle-like regions from multi-feature maps of different hierarchies and scales. Because the multi-feature maps combine the advantage of the deep and shallow convolutional layer, the first network performs well on locating the small targets in aerial image data. Then, the generated candidate regions are fed into the second network for feature extraction and decision making. Comprehensive experiments are conducted on the Vehicle Detection in Aerial Imagery (VEDAI) dataset and Munich vehicle dataset. The proposed cascaded detection model yields high performance, not only in detection accuracy but also in detection speed.},
	number = {12},
	urldate = {2018-02-18},
	journal = {Sensors (Basel, Switzerland)},
	author = {Zhong, Jiandan and Lei, Tao and Yao, Guangle},
	month = nov,
	year = {2017},
	pmid = {29186756},
	pmcid = {PMC5751529},
	file = {PubMed Central Full Text PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/HRZHNJR8/Zhong et al. - 2017 - Robust Vehicle Detection in Aerial Images Based on.pdf:application/pdf}
}

@article{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	urldate = {2018-02-19},
	journal = {arXiv:1604.07379 [cs]},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.07379},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	annote = {Comment: New results on ImageNet Generation},
	file = {arXiv\:1604.07379 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/88BAMQPP/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/INFFYUVI/1604.html:text/html}
}

@article{larsen_autoencoding_2015,
	title = {Autoencoding beyond pixels using a learned similarity metric},
	url = {http://arxiv.org/abs/1512.09300},
	abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
	urldate = {2018-02-19},
	journal = {arXiv:1512.09300 [cs, stat]},
	author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.09300},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1512.09300 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/VNU24CKM/Larsen et al. - 2015 - Autoencoding beyond pixels using a learned similar.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/TLR79RIX/1512.html:text/html}
}

@article{shelhamer_fully_2016,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1605.06211},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	urldate = {2018-02-19},
	journal = {arXiv:1605.06211 [cs]},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06211},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: to appear in PAMI (accepted May, 2016); journal edition of arXiv:1411.4038},
	file = {arXiv\:1605.06211 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/QMFQEGID/Shelhamer et al. - 2016 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/DD2TDPD2/1605.html:text/html}
}

@article{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2018-02-19},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {arXiv\:1505.04597 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/CFGKVUWS/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/6MJ5HG7P/1505.html:text/html}
}

@article{rezaei_conditional_2017,
	title = {Conditional {Adversarial} {Network} for {Semantic} {Segmentation} of {Brain} {Tumor}},
	url = {http://arxiv.org/abs/1708.05227},
	abstract = {Automated medical image analysis has a significant value in diagnosis and treatment of lesions. Brain tumors segmentation has a special importance and difficulty due to the difference in appearances and shapes of the different tumor regions in magnetic resonance images. Additionally, the data sets are heterogeneous and usually limited in size in comparison with the computer vision problems. The recently proposed adversarial training has shown promising results in generative image modeling. In this paper, we propose a novel end-to-end trainable architecture for brain tumor semantic segmentation through conditional adversarial training. We exploit conditional Generative Adversarial Network (cGAN) and train a semantic segmentation Convolution Neural Network (CNN) along with an adversarial network that discriminates segmentation maps coming from the ground truth or from the segmentation network for BraTS 2017 segmentation task[15, 4, 2, 3]. We also propose an end-to-end trainable CNN for survival day prediction based on deep learning techniques for BraTS 2017 prediction task [15, 4, 2, 3]. The experimental results demonstrate the superior ability of the proposed approach for both tasks. The proposed model achieves on validation data a DICE score, Sensitivity and Specificity respectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment system.},
	urldate = {2018-02-19},
	journal = {arXiv:1708.05227 [cs]},
	author = {Rezaei, Mina and Harmuth, Konstantin and Gierke, Willi and Kellermeier, Thomas and Fischer, Martin and Yang, Haojin and Meinel, Christoph},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05227},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to BraTS challenges which is part of MICCAI-2017},
	file = {arXiv\:1708.05227 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/XTKUTRBH/Rezaei et al. - 2017 - Conditional Adversarial Network for Semantic Segme.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/ADHYM6T8/1708.html:text/html}
}

@article{yang_automatic_2017,
	title = {Automatic {Liver} {Segmentation} {Using} an {Adversarial} {Image}-to-{Image} {Network}},
	url = {http://arxiv.org/abs/1707.08037},
	abstract = {Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.},
	urldate = {2018-02-19},
	journal = {arXiv:1707.08037 [cs]},
	author = {Yang, Dong and Xu, Daguang and Zhou, S. Kevin and Georgescu, Bogdan and Chen, Mingqing and Grbic, Sasa and Metaxas, Dimitris and Comaniciu, Dorin},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.08037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by MICCAI 2017},
	file = {arXiv\:1707.08037 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/KCGPHYDF/Yang et al. - 2017 - Automatic Liver Segmentation Using an Adversarial .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/G7GAXTFV/1707.html:text/html}
}

@article{xue_segan:_2017,
	title = {{SegAN}: {Adversarial} {Network} with {Multi}-scale \${L}\_1\$ {Loss} for {Medical} {Image} {Segmentation}},
	shorttitle = {{SegAN}},
	url = {http://arxiv.org/abs/1706.01805},
	abstract = {Inspired by classic generative adversarial networks (GAN), we propose a novel end-to-end adversarial neural network, called SegAN, for the task of medical image segmentation. Since image segmentation requires dense, pixel-level labeling, the single scalar real/fake output of a classic GAN's discriminator may be ineffective in producing stable and sufficient gradient feedback to the networks. Instead, we use a fully convolutional neural network as the segmentor to generate segmentation label maps, and propose a novel adversarial critic network with a multi-scale \$L\_1\$ loss function to force the critic and segmentor to learn both global and local features that capture long- and short-range spatial relationships between pixels. In our SegAN framework, the segmentor and critic networks are trained in an alternating fashion in a min-max game: The critic takes as input a pair of images, (original\_image \$*\$ predicted\_label\_map, original\_image \$*\$ ground\_truth\_label\_map), and then is trained by maximizing a multi-scale loss function; The segmentor is trained with only gradients passed along by the critic, with the aim to minimize the multi-scale loss function. We show that such a SegAN framework is more effective and stable for the segmentation task, and it leads to better performance than the state-of-the-art U-net segmentation method. We tested our SegAN method using datasets from the MICCAI BRATS brain tumor segmentation challenge. Extensive experimental results demonstrate the effectiveness of the proposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance comparable to the state-of-the-art for whole tumor and tumor core segmentation while achieves better precision and sensitivity for Gd-enhance tumor core segmentation; on BRATS 2015 SegAN achieves better performance than the state-of-the-art in both dice score and precision.},
	urldate = {2018-02-19},
	journal = {arXiv:1706.01805 [cs]},
	author = {Xue, Yuan and Xu, Tao and Zhang, Han and Long, Rodney and Huang, Xiaolei},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.01805},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1706.01805 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/2FRQNG84/Xue et al. - 2017 - SegAN Adversarial Network with Multi-scale \$L_1\$ .pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/CR9EKJ5J/1706.html:text/html}
}

@article{zheng_conditional_2015,
	title = {Conditional {Random} {Fields} as {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.03240},
	doi = {10.1109/ICCV.2015.179},
	abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.},
	urldate = {2018-02-20},
	journal = {arXiv:1502.03240 [cs]},
	author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
	month = dec,
	year = {2015},
	note = {arXiv: 1502.03240},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1529--1537},
	annote = {Comment: This paper is published in IEEE ICCV 2015},
	file = {arXiv\:1502.03240 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/UQAH7Y6B/Zheng et al. - 2015 - Conditional Random Fields as Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/9ULLA4ET/1502.html:text/html}
}

@article{schwing_fully_2015,
	title = {Fully {Connected} {Deep} {Structured} {Networks}},
	url = {http://arxiv.org/abs/1503.02351},
	abstract = {Convolutional neural networks with many layers have recently been shown to achieve excellent results on many high-level tasks such as image classification, object detection and more recently also semantic segmentation. Particularly for semantic segmentation, a two-stage procedure is often employed. Hereby, convolutional networks are trained to provide good local pixel-wise features for the second step being traditionally a more global graphical model. In this work we unify this two-stage process into a single joint training algorithm. We demonstrate our method on the semantic image segmentation task and show encouraging results on the challenging PASCAL VOC 2012 dataset.},
	urldate = {2018-02-20},
	journal = {arXiv:1503.02351 [cs]},
	author = {Schwing, Alexander G. and Urtasun, Raquel},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02351},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1503.02351 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/D7C73UIF/Schwing and Urtasun - 2015 - Fully Connected Deep Structured Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/P5M48QBA/1503.html:text/html}
}

@article{arnab_higher_2015,
	title = {Higher {Order} {Conditional} {Random} {Fields} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.08119},
	abstract = {We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.},
	urldate = {2018-02-20},
	journal = {arXiv:1511.08119 [cs]},
	author = {Arnab, Anurag and Jayasumana, Sadeep and Zheng, Shuai and Torr, Philip},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.08119},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2016},
	file = {arXiv\:1511.08119 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/P7XW4ILP/Arnab et al. - 2015 - Higher Order Conditional Random Fields in Deep Neu.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/CQR296Z6/1511.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2018-02-20},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1406.2661 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/EATLASEX/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/6CJYSFG4/1406.html:text/html}
}

@article{pinheiro_learning_2016,
	title = {Learning to {Refine} {Object} {Segments}},
	url = {http://arxiv.org/abs/1603.08695},
	abstract = {Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20\% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50\% faster than the original DeepMask network (under .8s per image).},
	urldate = {2018-02-20},
	journal = {arXiv:1603.08695 [cs]},
	author = {Pinheiro, Pedro O. and Lin, Tsung-Yi and Collobert, Ronan and Dollàr, Piotr},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.08695},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: extended version of ECCV camera-ready (figures 6-9 only in arXiv)},
	file = {arXiv\:1603.08695 PDF:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/PRH6UQQS/Pinheiro et al. - 2016 - Learning to Refine Object Segments.pdf:application/pdf;arXiv.org Snapshot:/home/joar/.zotero/zotero/97ry1s8l.default/zotero/storage/27MM46FC/1603.html:text/html}
}